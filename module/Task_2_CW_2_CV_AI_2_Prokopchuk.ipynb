{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Прокопчук Роман, ШІ-2\n",
    "\n",
    "Варіант 11\n",
    "- Задача 1: Зображення для знаходження відстані: KR2/ picture / . Зображення з відомою\n",
    "відстанню picture_1.jpg. Знаходження кутів: LOG, Знаходження ключових точок: SURF\n",
    "- Задача 2: Натренувати класифікатор на класах: `plate`, `elephant`, `mouse`\n",
    "\n",
    "# Задача 2"
   ],
   "metadata": {
    "id": "e5Ew-SgJne_w"
   }
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kk4KM6yIoSXB",
    "outputId": "c48fe88a-e0be-4573-905b-666c777f6f58"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "from google.colab import drive\n",
    "GOOGLE_DRIVE_ROOT = '/content/drive'\n",
    "drive.mount(GOOGLE_DRIVE_ROOT)"
   ]
  },
  {
   "metadata": {
    "id": "niT1-ToOovXX"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "import os\n",
    "\n",
    "ROOT_PATH_TO_DATA = os.path.join(GOOGLE_DRIVE_ROOT, \"MyDrive\", \"KNU_CV_CW_2\")\n",
    "TASK_2_PATH_TO_DATA = os.path.join(ROOT_PATH_TO_DATA, \"task_2\")"
   ]
  },
  {
   "metadata": {
    "id": "LeCJj7LTp92G"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "SUB_CLASSES = [\n",
    "    'plate',\n",
    "    'elephant',\n",
    "    'mouse'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "CIFAR100_ROOT = os.path.join(TASK_2_PATH_TO_DATA, \"cifar100\")"
   ],
   "metadata": {
    "id": "7xL9aVxCqfK2"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "class Cifar100Subset(Dataset):\n",
    "    def __init__(self, sub_classes, root, train=True, transform=None, download=False):\n",
    "        self.dataset = datasets.CIFAR100(root=root, train=train, transform=transform, download=download)\n",
    "        self.sub_classes = sub_classes\n",
    "        self.original_sub_classes_indices = [self.dataset.class_to_idx[name] for name in self.sub_classes]\n",
    "        self.target_indices = [i for i, target in enumerate(self.dataset.targets) if target in self.original_sub_classes_indices]\n",
    "        self.label_mapping = {orig_idx: new_idx for new_idx, orig_idx in enumerate(self.original_sub_classes_indices)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.target_indices[index]\n",
    "        image, original_label = self.dataset[index]\n",
    "        new_label = self.label_mapping[original_label]\n",
    "\n",
    "        return image, new_label\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "])\n",
    "\n",
    "cifar100_train = Cifar100Subset(\n",
    "    sub_classes=SUB_CLASSES,\n",
    "    root=CIFAR100_ROOT,\n",
    "    train=True,\n",
    "    transform=train_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "cifar100_test = Cifar100Subset(\n",
    "    sub_classes=SUB_CLASSES,\n",
    "    root=CIFAR100_ROOT,\n",
    "    train=False,\n",
    "    transform=test_transform,\n",
    "    download=True\n",
    ")"
   ],
   "metadata": {
    "id": "o_VbhbjnpLLU"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Cifar100CnnClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Cifar100CnnClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "1kfwNjcosQ-B"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "BATCH_SIZE=64"
   ],
   "metadata": {
    "id": "Lhm-_f2Mw2X1"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=cifar100_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=cifar100_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ],
   "metadata": {
    "id": "c5z42EUS15j2"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAhrXSwr2P4U",
    "outputId": "2cbb9e8b-31a9-4fa6-e8bf-078863ee1127"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, loader, criterion, optimizer, device=DEVICE):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    acc = 100 * correct / total\n",
    "\n",
    "    return avg_loss, acc"
   ],
   "metadata": {
    "id": "9QjLa17L4Jj9"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, loader, criterion, device=DEVICE):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    acc = 100 * correct / total\n",
    "\n",
    "    return avg_loss, acc"
   ],
   "metadata": {
    "id": "K-GZiiio4Y0O"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TASK_2_MODEL_OUTPUT = os.path.join(TASK_2_PATH_TO_DATA, \"model\")\n",
    "os.makedirs(TASK_2_MODEL_OUTPUT, exist_ok=True)"
   ],
   "metadata": {
    "id": "4QBqI5q77Tln"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 40"
   ],
   "metadata": {
    "id": "2YzJFZMy2JKC"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "model = Cifar100CnnClassifier(n_classes=len(SUB_CLASSES)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "start_time = time.time()\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"--------------Epoch {epoch + 1}/{N_EPOCHS} started--------------\")\n",
    "    train_loss, train_acc = train(model, train_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{N_EPOCHS}\",\n",
    "        f\"Train loss: {train_loss:.4f} | Train accuracy: {train_acc:.2f}%\",\n",
    "        f\"Test loss: {test_loss:.4f} | Test accuracy: {test_acc:.2f}%\",\n",
    "        f\"Time elapsed: {time.time() - epoch_start_time:.2f} sec\",\n",
    "        sep=\"\\n\"\n",
    "    )\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), os.path.join(TASK_2_MODEL_OUTPUT, \"best_model.pth\"))\n",
    "\n",
    "print(f\"Training time: {time.time() - start_time:.2f} sec\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrJreD7I3f-N",
    "outputId": "45344831-fd67-49b9-e6fa-7249bcca3d44"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------Epoch 1/40 started--------------\n",
      "Epoch 1/40\n",
      "Train loss: 1.1934 | Train accuracy: 47.73%\n",
      "Test loss: 0.9404 | Test accuracy: 51.33%\n",
      "Time elapsed: 1.29 sec\n",
      "--------------Epoch 2/40 started--------------\n",
      "Epoch 2/40\n",
      "Train loss: 0.8328 | Train accuracy: 59.87%\n",
      "Test loss: 1.1587 | Test accuracy: 46.67%\n",
      "Time elapsed: 1.65 sec\n",
      "--------------Epoch 3/40 started--------------\n",
      "Epoch 3/40\n",
      "Train loss: 0.7382 | Train accuracy: 68.53%\n",
      "Test loss: 0.8905 | Test accuracy: 66.67%\n",
      "Time elapsed: 1.90 sec\n",
      "--------------Epoch 4/40 started--------------\n",
      "Epoch 4/40\n",
      "Train loss: 0.6716 | Train accuracy: 71.87%\n",
      "Test loss: 0.6748 | Test accuracy: 67.67%\n",
      "Time elapsed: 0.84 sec\n",
      "--------------Epoch 5/40 started--------------\n",
      "Epoch 5/40\n",
      "Train loss: 0.6554 | Train accuracy: 70.27%\n",
      "Test loss: 0.6993 | Test accuracy: 72.00%\n",
      "Time elapsed: 0.80 sec\n",
      "--------------Epoch 6/40 started--------------\n",
      "Epoch 6/40\n",
      "Train loss: 0.6175 | Train accuracy: 73.53%\n",
      "Test loss: 0.5699 | Test accuracy: 77.67%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 7/40 started--------------\n",
      "Epoch 7/40\n",
      "Train loss: 0.5535 | Train accuracy: 77.20%\n",
      "Test loss: 0.5032 | Test accuracy: 77.67%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 8/40 started--------------\n",
      "Epoch 8/40\n",
      "Train loss: 0.5419 | Train accuracy: 77.67%\n",
      "Test loss: 0.5662 | Test accuracy: 76.33%\n",
      "Time elapsed: 0.82 sec\n",
      "--------------Epoch 9/40 started--------------\n",
      "Epoch 9/40\n",
      "Train loss: 0.5203 | Train accuracy: 78.80%\n",
      "Test loss: 0.4463 | Test accuracy: 82.67%\n",
      "Time elapsed: 0.82 sec\n",
      "--------------Epoch 10/40 started--------------\n",
      "Epoch 10/40\n",
      "Train loss: 0.4398 | Train accuracy: 81.87%\n",
      "Test loss: 0.3898 | Test accuracy: 85.00%\n",
      "Time elapsed: 0.87 sec\n",
      "--------------Epoch 11/40 started--------------\n",
      "Epoch 11/40\n",
      "Train loss: 0.4241 | Train accuracy: 83.13%\n",
      "Test loss: 0.5484 | Test accuracy: 78.33%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 12/40 started--------------\n",
      "Epoch 12/40\n",
      "Train loss: 0.3946 | Train accuracy: 83.80%\n",
      "Test loss: 0.3047 | Test accuracy: 88.67%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 13/40 started--------------\n",
      "Epoch 13/40\n",
      "Train loss: 0.3874 | Train accuracy: 84.53%\n",
      "Test loss: 0.3217 | Test accuracy: 86.33%\n",
      "Time elapsed: 0.82 sec\n",
      "--------------Epoch 14/40 started--------------\n",
      "Epoch 14/40\n",
      "Train loss: 0.3731 | Train accuracy: 85.20%\n",
      "Test loss: 0.2767 | Test accuracy: 89.00%\n",
      "Time elapsed: 0.87 sec\n",
      "--------------Epoch 15/40 started--------------\n",
      "Epoch 15/40\n",
      "Train loss: 0.3316 | Train accuracy: 87.33%\n",
      "Test loss: 0.3156 | Test accuracy: 88.00%\n",
      "Time elapsed: 1.13 sec\n",
      "--------------Epoch 16/40 started--------------\n",
      "Epoch 16/40\n",
      "Train loss: 0.2975 | Train accuracy: 88.53%\n",
      "Test loss: 0.3005 | Test accuracy: 88.00%\n",
      "Time elapsed: 1.25 sec\n",
      "--------------Epoch 17/40 started--------------\n",
      "Epoch 17/40\n",
      "Train loss: 0.2802 | Train accuracy: 88.67%\n",
      "Test loss: 0.4652 | Test accuracy: 83.00%\n",
      "Time elapsed: 1.22 sec\n",
      "--------------Epoch 18/40 started--------------\n",
      "Epoch 18/40\n",
      "Train loss: 0.3043 | Train accuracy: 88.73%\n",
      "Test loss: 0.2712 | Test accuracy: 89.00%\n",
      "Time elapsed: 0.98 sec\n",
      "--------------Epoch 19/40 started--------------\n",
      "Epoch 19/40\n",
      "Train loss: 0.2710 | Train accuracy: 89.87%\n",
      "Test loss: 0.3186 | Test accuracy: 87.67%\n",
      "Time elapsed: 0.81 sec\n",
      "--------------Epoch 20/40 started--------------\n",
      "Epoch 20/40\n",
      "Train loss: 0.2503 | Train accuracy: 90.20%\n",
      "Test loss: 0.3527 | Test accuracy: 89.00%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 21/40 started--------------\n",
      "Epoch 21/40\n",
      "Train loss: 0.2439 | Train accuracy: 91.60%\n",
      "Test loss: 0.4019 | Test accuracy: 85.67%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 22/40 started--------------\n",
      "Epoch 22/40\n",
      "Train loss: 0.2393 | Train accuracy: 91.13%\n",
      "Test loss: 0.3125 | Test accuracy: 88.67%\n",
      "Time elapsed: 1.09 sec\n",
      "--------------Epoch 23/40 started--------------\n",
      "Epoch 23/40\n",
      "Train loss: 0.2079 | Train accuracy: 92.73%\n",
      "Test loss: 0.2502 | Test accuracy: 90.67%\n",
      "Time elapsed: 0.91 sec\n",
      "--------------Epoch 24/40 started--------------\n",
      "Epoch 24/40\n",
      "Train loss: 0.1949 | Train accuracy: 92.07%\n",
      "Test loss: 0.3330 | Test accuracy: 87.33%\n",
      "Time elapsed: 0.82 sec\n",
      "--------------Epoch 25/40 started--------------\n",
      "Epoch 25/40\n",
      "Train loss: 0.2052 | Train accuracy: 91.73%\n",
      "Test loss: 0.3937 | Test accuracy: 85.00%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 26/40 started--------------\n",
      "Epoch 26/40\n",
      "Train loss: 0.2105 | Train accuracy: 92.07%\n",
      "Test loss: 0.6474 | Test accuracy: 84.00%\n",
      "Time elapsed: 0.84 sec\n",
      "--------------Epoch 27/40 started--------------\n",
      "Epoch 27/40\n",
      "Train loss: 0.2441 | Train accuracy: 91.40%\n",
      "Test loss: 0.2820 | Test accuracy: 88.67%\n",
      "Time elapsed: 0.80 sec\n",
      "--------------Epoch 28/40 started--------------\n",
      "Epoch 28/40\n",
      "Train loss: 0.2192 | Train accuracy: 92.20%\n",
      "Test loss: 0.2557 | Test accuracy: 91.00%\n",
      "Time elapsed: 0.78 sec\n",
      "--------------Epoch 29/40 started--------------\n",
      "Epoch 29/40\n",
      "Train loss: 0.2045 | Train accuracy: 92.73%\n",
      "Test loss: 0.2623 | Test accuracy: 89.67%\n",
      "Time elapsed: 0.87 sec\n",
      "--------------Epoch 30/40 started--------------\n",
      "Epoch 30/40\n",
      "Train loss: 0.1714 | Train accuracy: 93.33%\n",
      "Test loss: 0.3616 | Test accuracy: 87.00%\n",
      "Time elapsed: 1.26 sec\n",
      "--------------Epoch 31/40 started--------------\n",
      "Epoch 31/40\n",
      "Train loss: 0.2073 | Train accuracy: 92.47%\n",
      "Test loss: 0.3223 | Test accuracy: 87.67%\n",
      "Time elapsed: 1.27 sec\n",
      "--------------Epoch 32/40 started--------------\n",
      "Epoch 32/40\n",
      "Train loss: 0.1613 | Train accuracy: 93.33%\n",
      "Test loss: 0.3337 | Test accuracy: 88.33%\n",
      "Time elapsed: 1.22 sec\n",
      "--------------Epoch 33/40 started--------------\n",
      "Epoch 33/40\n",
      "Train loss: 0.1363 | Train accuracy: 94.87%\n",
      "Test loss: 0.3342 | Test accuracy: 90.67%\n",
      "Time elapsed: 0.81 sec\n",
      "--------------Epoch 34/40 started--------------\n",
      "Epoch 34/40\n",
      "Train loss: 0.1512 | Train accuracy: 94.33%\n",
      "Test loss: 0.4848 | Test accuracy: 85.00%\n",
      "Time elapsed: 0.82 sec\n",
      "--------------Epoch 35/40 started--------------\n",
      "Epoch 35/40\n",
      "Train loss: 0.1387 | Train accuracy: 94.33%\n",
      "Test loss: 0.3598 | Test accuracy: 87.67%\n",
      "Time elapsed: 1.11 sec\n",
      "--------------Epoch 36/40 started--------------\n",
      "Epoch 36/40\n",
      "Train loss: 0.1568 | Train accuracy: 94.67%\n",
      "Test loss: 0.2885 | Test accuracy: 90.33%\n",
      "Time elapsed: 0.84 sec\n",
      "--------------Epoch 37/40 started--------------\n",
      "Epoch 37/40\n",
      "Train loss: 0.1521 | Train accuracy: 94.60%\n",
      "Test loss: 0.2619 | Test accuracy: 91.33%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 38/40 started--------------\n",
      "Epoch 38/40\n",
      "Train loss: 0.1436 | Train accuracy: 94.67%\n",
      "Test loss: 0.1960 | Test accuracy: 92.00%\n",
      "Time elapsed: 0.82 sec\n",
      "--------------Epoch 39/40 started--------------\n",
      "Epoch 39/40\n",
      "Train loss: 0.1057 | Train accuracy: 96.07%\n",
      "Test loss: 0.2953 | Test accuracy: 92.33%\n",
      "Time elapsed: 0.83 sec\n",
      "--------------Epoch 40/40 started--------------\n",
      "Epoch 40/40\n",
      "Train loss: 0.1099 | Train accuracy: 95.60%\n",
      "Test loss: 0.2836 | Test accuracy: 90.00%\n",
      "Time elapsed: 0.88 sec\n",
      "Training time: 39.63 sec\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model(model, os.path.join(TASK_2_MODEL_OUTPUT, \"best_model.pth\")).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "final_test_dataloader = torch.utils.data.DataLoader(cifar100_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in final_test_dataloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')"
   ],
   "metadata": {
    "id": "v_CrC8btxj4O",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "77522c40-6cb3-49de-966f-0fbbe4ce4c7e"
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the test images: 92.33%\n"
     ]
    }
   ]
  }
 ]
}
